{
  "model": "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4",
  "gpu_memory_utilization": 0.70,
  "tensor_parallel_size": 2,
  "enforce_eager": "true",
  "max_model_len": 4096
}
