{
  "model": "Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4",
  "gpu_memory_utilization": 0.6,
  "tensor_parallel_size": 2,
  "max_model_len": 8192,
  "max_num_batched_tokens": 2048,
  "block_size": 16,
  "enable_chunked_prefill": true,
  "cpu_offload_gb": 2
}
